{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the training data\n",
        "with open('train-claims.json') as file:\n",
        "    train_data = json.load(file)\n",
        "\n",
        "# Collect claims texts and labels\n",
        "claims = [details['claim_text'] for details in train_data.values()]\n",
        "labels = [details['claim_label'] for details in train_data.values()]\n",
        "\n",
        "# Create a mapping for labels to integers\n",
        "unique_labels = list(set(labels))\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Convert labels to indices\n",
        "label_indices = [label_to_index[label] for label in labels]\n",
        "\n",
        "# Convert label indices to one-hot encoding\n",
        "label_seq = to_categorical(label_indices)\n",
        "\n",
        "# Tokenize and pad claims\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(claims)\n",
        "sequences = tokenizer.texts_to_sequences(claims)\n",
        "padded_sequences = pad_sequences(sequences, padding='post', maxlen=50)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, label_seq, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, label_seq, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the training data and evidence data\n",
        "with open('train-claims.json') as file:\n",
        "    train_data = json.load(file)\n",
        "\n",
        "with open('evidence.json') as file:  # Ensure the path to evidence.json is correct\n",
        "    evidence_data = json.load(file)\n",
        "\n",
        "# Function to fetch evidence texts based on evidence IDs\n",
        "def get_evidence_text(evidence_ids):\n",
        "    # Retrieve and concatenate evidence texts corresponding to the evidence IDs\n",
        "    return ' '.join([evidence_data.get(eid, '') for eid in evidence_ids])\n",
        "\n",
        "# Prepare data by combining claims with their corresponding evidence\n",
        "combined_texts = []\n",
        "labels = []\n",
        "for claim_id, claim_details in train_data.items():\n",
        "    # Fetch the evidence texts for the claim\n",
        "    evidence_text = get_evidence_text(claim_details['evidences'])\n",
        "    # Combine the claim text with the evidence texts\n",
        "    combined_text = claim_details['claim_text'] + \" \" + evidence_text\n",
        "    combined_texts.append(combined_text)\n",
        "    labels.append(claim_details['claim_label'])\n",
        "\n",
        "# Convert labels to categorical\n",
        "unique_labels = list(set(labels))\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "label_indices = [label_to_index[label] for label in labels]\n",
        "label_seq = to_categorical(label_indices)\n",
        "\n",
        "# Tokenize and pad combined texts\n",
        "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(combined_texts)\n",
        "sequences = tokenizer.texts_to_sequences(combined_texts)\n",
        "padded_sequences = pad_sequences(sequences, padding='post', maxlen=50)\n",
        "\n",
        "# No need to split the data as we will use the entire dataset for training\n",
        "X_train = padded_sequences\n",
        "y_train = label_seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 50, 128)           2560000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 128)           131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,745,412\n",
            "Trainable params: 2,745,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "39/39 [==============================] - 7s 52ms/step - loss: 1.3006 - accuracy: 0.4194\n",
            "Epoch 2/10\n",
            "39/39 [==============================] - 2s 53ms/step - loss: 1.1961 - accuracy: 0.4226\n",
            "Epoch 3/10\n",
            "39/39 [==============================] - 2s 49ms/step - loss: 0.9169 - accuracy: 0.6075\n",
            "Epoch 4/10\n",
            "39/39 [==============================] - 2s 52ms/step - loss: 0.6034 - accuracy: 0.7419\n",
            "Epoch 5/10\n",
            "39/39 [==============================] - 2s 51ms/step - loss: 0.4547 - accuracy: 0.8184\n",
            "Epoch 6/10\n",
            "39/39 [==============================] - 2s 50ms/step - loss: 0.3491 - accuracy: 0.8762\n",
            "Epoch 7/10\n",
            "39/39 [==============================] - 2s 47ms/step - loss: 0.2602 - accuracy: 0.9072\n",
            "Epoch 8/10\n",
            "39/39 [==============================] - 2s 49ms/step - loss: 0.2034 - accuracy: 0.9300\n",
            "Epoch 9/10\n",
            "39/39 [==============================] - 2s 50ms/step - loss: 0.1879 - accuracy: 0.9340\n",
            "Epoch 10/10\n",
            "39/39 [==============================] - 2s 47ms/step - loss: 0.1958 - accuracy: 0.9275\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "with tf.device('/cpu:0'):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=20000, output_dim=128, input_length=50,mask_zero=True),\n",
        "        LSTM(128, return_sequences=True),\n",
        "        LSTM(64),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(label_seq.shape[1], activation='softmax')  # Output layer\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 2s 16ms/step - loss: 1.9599 - accuracy: 0.4286\n",
            "Model Loss: 1.9598561525344849, Model Accuracy: 0.4285714328289032\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load the development claims\n",
        "with open('dev-claims.json') as file:\n",
        "    dev_data = json.load(file)\n",
        "\n",
        "# Assuming 'evidence.json' has been loaded as evidence_data (you should have this from your earlier steps)\n",
        "# Reuse the get_evidence_text function if available, else define it again\n",
        "def get_evidence_text(evidence_ids):\n",
        "    return ' '.join([evidence_data.get(eid, '') for eid in evidence_ids])\n",
        "\n",
        "# Prepare the combined texts and labels for the development set\n",
        "dev_combined_texts = []\n",
        "dev_labels = []\n",
        "for claim_id, claim_details in dev_data.items():\n",
        "    evidence_text = get_evidence_text(claim_details['evidences'])\n",
        "    combined_text = claim_details['claim_text'] + \" \" + evidence_text\n",
        "    dev_combined_texts.append(combined_text)\n",
        "    dev_labels.append(claim_details['claim_label'])\n",
        "\n",
        "# Convert labels of the development set to categorical\n",
        "dev_label_indices = [label_to_index[label] for label in dev_labels]\n",
        "dev_label_seq = to_categorical(dev_label_indices)\n",
        "\n",
        "# Tokenize and pad the development combined texts\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_combined_texts)\n",
        "dev_padded_sequences = pad_sequences(dev_sequences, padding='post', maxlen=50)\n",
        "\n",
        "# Evaluate the model on the development set\n",
        "with tf.device('/cpu:0'):\n",
        "    evaluation = model.evaluate(dev_padded_sequences, dev_label_seq)\n",
        "    print(f'Model Loss: {evaluation[0]}, Model Accuracy: {evaluation[1]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 2s 14ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get model predictions\n",
        "with tf.device('/cpu:0'):\n",
        "    predictions = model.predict(dev_padded_sequences)\n",
        "    predicted_labels_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Convert categorical labels back to label indices for comparison\n",
        "true_labels_indices = np.argmax(dev_label_seq, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claim ID: claim-752\n",
            "Claim Text: [South Australia] has the most expensive electricity in the world. [citation needed] South Australia has the highest retail price for electricity in the country. \"South Australia has the highest power prices in the world\".\n",
            "Predicted Label: NOT_ENOUGH_INFO\n",
            "True Label: SUPPORTS\n",
            "----\n",
            "Claim ID: claim-375\n",
            "Claim Text: when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod­uces 1.3 per cent of this 3 per cent, then no amount of emissions reductio­n here will have any effect on global climate. The 2011 UNEP Green Economy report states that \"[a]agricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions. With a market share of 30% and (potentially) clean electricity, heat pumps could reduce global CO 2 emissions by 8% annually. In the modern era, emissions to the atmosphere from volcanoes are approximately 0.645 billion tonnes of CO 2 per year, whereas humans contribute 29 billion tonnes of CO 2 each year. Cumulative anthropogenic (i.e., human-emitted) emissions of CO 2 from fossil fuel use are a major cause of global warming, and give some indication of which countries have contributed most to human-induced climate change. Other countries with fast growing emissions are South Korea, Iran, and Australia (which apart from the oil rich Persian Gulf states, now has the highest percapita emission rate in the world).\n",
            "Predicted Label: REFUTES\n",
            "True Label: NOT_ENOUGH_INFO\n",
            "----\n",
            "Claim ID: claim-1266\n",
            "Claim Text: This means that the world is now 1C warmer than it was in pre-industrial times Multiple independently produced instrumental datasets confirm that the 2009–2018 decade was 0.93 ± 0.07 °C warmer than the pre-industrial baseline (1850–1900). The planet is now 0.8 °C warmer than in pre-industrial times.\n",
            "Predicted Label: REFUTES\n",
            "True Label: SUPPORTS\n",
            "----\n",
            "Claim ID: claim-2164\n",
            "Claim Text: Greenland has only lost a tiny fraction of its ice mass If iceberg calving has happened as an average, Greenland lost 294 Gt of its mass during 2007 (one km3 of ice weighs about 0.9 Gt). Findings show that Greenland has lost 3.8 trillion tonnes of ice since 1992, enough to raise sea levels by almost 11mm (1.06cm). \"Greenland Glaciers Losing Ice Much Faster, Study Says\". Between then and 2010, the mountain lost 80 percent of its ice — two-thirds of which since another scientific expedition in the 1970s.\n",
            "Predicted Label: SUPPORTS\n",
            "True Label: REFUTES\n",
            "----\n",
            "Claim ID: claim-1607\n",
            "Claim Text: CO2 limits won't cool the planet. Less energy reaches the upper atmosphere, which is therefore cooler because of this absorption. Occupational CO 2 exposure limits have been set in the United States at 0.5% (5000 ppm) for an eight-hour period. As the temperature rises closer to the value the white daisies like, the white daisies outreproduce the black daisies, leading to a larger percentage of white surface, and more sunlight is reflected, reducing the heat input and eventually cooling the planet. Global warming is the long-term rise in the average temperature of the Earth's climate system. If cloud cover increases, more sunlight will be reflected back into space, cooling the planet.\n",
            "Predicted Label: REFUTES\n",
            "True Label: NOT_ENOUGH_INFO\n",
            "----\n",
            "Claim ID: claim-1273\n",
            "Claim Text: The rapid changes in the climate may have profound consequences for humans and other species… Severe drought caused food shortages for millions of people in Ethiopia, with a lack of rainfall resulting in “intense and widespread” forest fires in Indonesia that belched out a vast quantity of greenhouse gas According to the WWF, the combination of climate change and deforestation increases the drying effect of dead trees that fuels forest fires. Recurring droughts leading to desertification in East Africa have created grave ecological catastrophes, prompting food shortages in 1984–85, 2006 and 2011. A 17-year-long civil war, along with severe drought, negatively impacted Ethiopia's environmental conditions, leading to even greater habitat degradation. A primary cause of the famine (one of the largest seen in the country) is that Ethiopia (and the surrounding Horn) was still recovering from the droughts which occurred in the mid-late 1970s.\n",
            "Predicted Label: NOT_ENOUGH_INFO\n",
            "True Label: SUPPORTS\n",
            "----\n",
            "Claim ID: claim-2580\n",
            "Claim Text: Volcanoes emit around 0.3 billion tonnes of CO2 per year. The volcano produces about 400,000 ± 100,000 tonnes per year (400,000 ± 100,000 t/a) of CO 2, which is about 0.2–0.6% of the worldwide CO 2 flux at subaerial volcanoes. Volcanic activity releases about 130 to 230 teragrams (145 million to 255 million short tons) of carbon dioxide each year.\n",
            "Predicted Label: NOT_ENOUGH_INFO\n",
            "True Label: DISPUTED\n",
            "----\n",
            "Claim ID: claim-75\n",
            "Claim Text: The science is clear, climate change is making extreme weather events, including tornadoes, worse. The main impact of global warming on the weather is an increase in extreme weather events such as heat waves, droughts, cyclones, blizzards and rainstorms. Documented long-term climate changes include changes in Arctic temperatures and ice, widespread changes in precipitation amounts, ocean salinity, wind patterns and extreme weather including droughts, heavy precipitation, heat waves and the intensity of tropical cyclones. As the Earth's climate warms, we are seeing many changes: stronger, more destructive hurricanes; heavier rainfall; more disastrous flooding; more areas of the world experiencing severe drought; and more heat waves.\" This could lead to changing, and for all emissions scenarios more unpredictable, weather patterns around the world, less frost days, more extreme events (droughts and storm or flood disasters), and warmer sea temperatures and melting glaciers causing sea levels to rise. Climate change caused by human activities that emit greenhouse gases into the air is expected to affect the frequency of extreme weather events such as drought, extreme temperatures, flooding, high winds, and severe storms.\n",
            "Predicted Label: REFUTES\n",
            "True Label: SUPPORTS\n",
            "----\n",
            "Claim ID: claim-2813\n",
            "Claim Text: In fact, the authors go on to estimate climate sensitivity from their findings, calculate a value between 2.3 to 4.1°C. The IPCC literature assessment estimates that TCR likely lies between 1 °C and 2.5 °C. For constant humidity they computed a climate sensitivity of 2.3 °C per doubling of CO2 (which they rounded to 2, the value most often quoted from their work, in the abstract of the paper). The 1990 IPCC First Assessment Report estimated that equilibrium climate sensitivity to a doubling of CO 2 lay between 1.5 and 4.5 °C (2.7 and 8.1 °F), with a \"best guess in the light of current knowledge\" of 2.5 °C (4.5 °F). IPCC authors concluded ECS is very likely to be greater than 1.5 °C (2.7 °F) and likely to lie in the range 2 to 4.5 °C (4 to 8.1 °F), with a most likely value of about 3 °C (5 °F). The IPCC Fifth Assessment Report reverted to the earlier range of 1.5 to 4.5 °C (2.7 to 8.1 °F) (high confidence) because some estimates using industrial-age data came out low.\n",
            "Predicted Label: SUPPORTS\n",
            "True Label: DISPUTED\n",
            "----\n",
            "Claim ID: claim-2335\n",
            "Claim Text: Satellite measurements of infrared spectra over the past 40 years observe less energy escaping to space at the wavelengths associated with CO2. Cosmic-temperature measurements The VLT has detected, for the first time, carbon-monoxide molecules in a galaxy located almost 11 billion light-years away. The LWIR (8–15 μm) region is especially useful since some radiation at these wavelengths can escape into space through the atmosphere. The three channels use the same frequency but different carbon dioxide cell pressure, the corresponding weighting functions peaks at 29 km for channel 1, 37 km for channel 2 and 45 km for channel 3. The process of deriving trends from SSUs measurement has proved particularly difficult because of satellites drift, inter-calibration between different satellite with scant overlap and gas leak in the instrument carbon dioxide pressure cell, furthermore since the radiances measured by SSUs are due to emission by carbon dioxide the weighting functions move to higher altitudes as the carbon dioxide concentration in the stratosphere increase. They measure radiances in various wavelength bands.\n",
            "Predicted Label: SUPPORTS\n",
            "True Label: NOT_ENOUGH_INFO\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "misclassified_indices = np.where(predicted_labels_indices != true_labels_indices)[0]\n",
        "# Assuming you have a way to map label indices back to label names\n",
        "index_to_label = {v: k for k, v in label_to_index.items()}\n",
        "\n",
        "for index in misclassified_indices[:10]:  # Limiting to the first 10 errors for initial review\n",
        "    print(\"Claim ID:\", list(dev_data.keys())[index])\n",
        "    print(\"Claim Text:\", dev_combined_texts[index])\n",
        "    print(\"Predicted Label:\", index_to_label[predicted_labels_indices[index]])\n",
        "    print(\"True Label:\", index_to_label[true_labels_indices[index]])\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
